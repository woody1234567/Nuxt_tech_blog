---
title: RAG 系統設計全解析：讓 LLM 學會查資料再說話
description: 深入探討 Retrieval-Augmented Generation (RAG) 的架構設計，從資料清洗、切片策略到檢索優化，完整理解如何打造精準且可信的 AI 知識庫。
tags: [rag, llm, ai, vector-database, system-design]
category: ai
date: 2026-02-19
---

# RAG 系統設計全解析（Retrieval-Augmented Generation）

在大型語言模型（LLM）被廣泛應用之後，一個核心問題逐漸浮現：模型很會「說」，但不一定知道「最新」或「專屬於你的私有資料」。這時候，**RAG（Retrieval-Augmented Generation）** 就成為關鍵架構。

一句話理解 RAG：
> **RAG = 先查資料（Retrieval），再生成答案（Generation）**

它讓 LLM 不只依賴預訓練記憶，而是可以「動態讀取外部知識」，讓模型變成一個結合了「推理引擎」與「即時知識查詢」的強大系統。

---

## 一、RAG 在解決什麼問題？

LLM 本身存在以下限制：
- **無法存取私有資料**：如公司內部合約、個人筆記。
- **知識過時**：無法知道訓練切斷點之後發生的事。
- **產生幻覺（Hallucination）**：一本正經地胡說八道。
- **無法引用來源**：難以驗證答案的真實性。

RAG 的運作流程如下：
1. **使用者提問**
2. **搜尋相關文件**
3. **把文件內容與問題一起餵給 LLM**
4. **生成有依據的回答**

---

## 二、RAG 的五層核心架構

一個標準的 RAG 系統可以拆解為以下五層：

1.  **資料層 (Data Layer)**：處理原始格式（PDF, Markdown, Notion, API 等）。
2.  **索引層 (Indexing Layer)**：將資料清洗、切片並轉為向量儲存。
3.  **檢索層 (Retrieval Layer)**：根據問題尋找最相關的資料片段。
4.  **Prompt 組裝層 (Augmentation Layer)**：將檢索結果與問題包裝成給模型的指令。
5.  **生成層 (Generation Layer)**：由 LLM 產出最終答案。

---

## 三、關鍵設計：Chunking（資料切片）

Chunk 是 RAG 成敗的核心。資料切得太細會失去上下文，切得太粗則會包含過多雜訊並浪費 Token。

**常見策略：**
- **固定大小**：如每 500 tokens 一個 chunk，並保留 10% 的重複區域（Overlap）以維持連貫性。
- **語義切片 (Semantic Chunking)**：依照段落、標題或語意改變點來切換。
- **層級化檢索 (Parent-Child Retrieval)**：搜尋小片段，但提供給模型大段落作為背景。

---

## 四、檢索策略優化

為了提高回答的精準度，現代 RAG 系統常採用以下技術：

- **混合搜尋 (Hybrid Search)**：結合傳統關鍵字搜尋（BM25）與語意搜尋（Vector Search）。
- **重排序 (Reranking)**：先取回 20 個候選片段，再用精準模型重新評分，選出前 5 個。
- **多查詢檢索 (Multi-query Retrieval)**：讓 LLM 把使用者的問題改寫成 3 個變體，擴大搜尋範圍以提高命中率。

---

## 五、如何降低幻覺？

在 Prompt 組裝層，我們可以透過以下設計來控制模型行為：
- **限制回答範圍**：要求模型「若資料不足，請回答不知道，不要自行發揮」。
- **強制引用來源**：要求模型在回答中標註參考的文件編號或段列。
- **驗證機制**：檢查生成內容是否真的能在檢索到的 Chunk 中找到對應依據。

---

## 結論

RAG 的本質不是讓模型變聰明，而是 **讓模型學會查資料再說話**。

當資料品質高、檢索準確且 Prompt 控制良好時，RAG 系統會比單純的 LLM 穩定且實用許多。無論是建立企業知識庫、個人化 AI 教練，還是技術文件查詢系統，RAG 都是目前的最佳實踐架構。
